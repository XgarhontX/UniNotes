# Eager vs Lazy Learning
- ![[Pasted image 20230116175851.png|150]] 
- ![[Pasted image 20230116180043.png|150]] (everything we learn in this class is Eager, except kNN)
# k-Nearest Neighbor
- ![[Pasted image 20230116180412.png|200]]
	- a. x is inferred as negative
	- c. x is inferred as positive by majority count, or negative by distance
	- b. there is a tie, we can pick randomly what x is
- Larger k, less chance for overfitting
- Image describing algorithm:
	- ![[Pasted image 20230116180753.png|200]]
## Instance-Based Learning
- ![[Pasted image 20230116180826.png|300]]![[Pasted image 20230116180918.png|200]]
	- This is the traditional k-Nearest Neighbor algorithm
		- We can mean the votes values to determine the new data.
		- No training, fast but easily fooled by outliers and bad attributes
## Complex Decision Boundaries
- Voronoi Cell
	- ![[Pasted image 20230116181224.png|250]]
		- ![[Pasted image 20230116181245.png|100]]
- Example usage:
	- ![[Pasted image 20230116181350.png|400]]
		- NN & 5-NN classifier: Shows where all new data will be classified as when added.
			- Those are decision boundaries, very complex.
			- In NN, we have overfitting pockets of classification boundaries (e.g. small green in top left red)
			- In 5-NN, we have ties as those white spots. We just have randomize the classification. 
# Distance-Weighted k-Nearest Neighbor
- Instead of just doing some arbitrary $k$, what if we want $k$ to be the full data set?
	- We can do it with Distance Based & each data instance's Weighted Average
- Most likely, nearer neighbors to weigh more, giving us:
	- ![[Pasted image 20230116224403.png|250]]
		- $w_i$ is the weight
		- $d$ = distance measure between
			- $x_q$ = query data instance
			- $x_i$ = training data instance
	- Data with further distance will not contribute to the classification as much
## Distance Measure
- Data will vary in their features.
	- You might need more than one measure at once depending on your features
### Numeric Features for Distance Measure
- ![[Pasted image 20230116185814.png|200]]
	- x is the training instance
	- y is the test instance
	- m is the count of features
	- If m is 2:
		- ![[Pasted image 20230116190140.png|200]]![[Pasted image 20230116190810.png|400]]
### Symbolic Features for Distance Measure
- ![[Pasted image 20230116190605.png|250]]
	- Hamming Distance is simply counting how many features are different between 2 data instances
### Boolean Features for Distance Measure
- ![[Pasted image 20230116223028.png|200]]
	- Applying the previously discussed measures will make all data similar to each other, since there will be lots of 0 (false) usually, making the distance small.
		- So, Jaccard Distance look only at the 1s.
		- ![[Pasted image 20230116223304.png|225]] (count of columns $\land$ / count of columns $\lor$)
## Application of Distance-Weighted k-Nearest Neighbor
- What price should I market my house?
- Recommendation (e-commerce, recommendation) (aka Collaborative Filtering)
	- ![[Pasted image 20230116230532.png|400]]
		- $a$ is the new user to predict if they will like
		- $i$ is the known users' rating of the content 
		- The formula is changed with 
			- $\bar{r}_a$ & $\bar{r}_u$
				- As some users give low ratings and some give high ratings, we want to offset/adjust to their rating scale through avg.
			- $w_{a,u}$ is the Pearson's Correlation Coefficient (another Distance Measure)
				- ![[Pasted image 20230116231000.png|100]]
# Curse of Dimensionality
- Too many contributing irrelevant features
- Not enough relevant features
- The other big problem in Machine Learning along with Overfitting
- Applying to kNN
	- The higher the dimensions (attributes count), the 