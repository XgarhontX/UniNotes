# Picture Exercise
- ![[Pasted image 20230105145756.png|200]]
	- Height & Lengths (features, w/ dimensionality of 2)
		- of Dogs and Cats
	- 2 Decision Boundary (aka models, classifiers, hypotheses)
		- Complex (Red)
			- Probably overfitting
		- And some non linear line (Black)
			- (it could be a non linear regression line)
- Machine Learning needs
	- training data
	- bias
		- restriction: restrict possible model
		- search: restrict hypothesis search pace
# Supervised Learning
- ![[Pasted image 20230105150540.png|350]]
	- $<x, f(x)>$
		- $x$ = input feature vector
		- $f(x)$ = output (can be vector too, if model is made for it)
- Examples
	- ![[Pasted image 20230105150828.png|250]]
		- If you can find $f(x)$, then don't use a Machine Learning. Machine Learning is for approximating & inter/extrapolating
## Appropriate Applications for Supervised Learning
- ![[Pasted image 20230105151114.png|400]]
## A Learning Problem
- ![[Pasted image 20230105151536.png|200]]
	- $x$ = 4 boolean inputs
		- Fake example:
			- $x_1$ = it's a weekday
			- $x_2$ = it's morning
			- $x_3$ = there a faculty meeting
			- $x_4$ = it's summer
		- $n$ = 4, so there are $2^{n}=2^4$ = 16 possible input instances
	- $y$ = $f(x)$ = is the boolean output
		- Fake example:
			- $y$ = Prof Martine is on campus
		- There are $2^{2^n}=2^{2^4}=2^{16}$ = (doubly exponential) possible functions that will cover all patterns of outputs (aka Complete Hypothesis Space).
			- However, from one training example, we bring down the possible functions by 1/2, so $2^{15}$
				- Again, and we bring it down to $2^{14}$
				- So with 7 examples, we cut down to $2^{9}$
	- ML is used to find the Unknown Function
### Hypothesis Space
- ![[Pasted image 20230105152606.png|400]]
	- Having training data alone is not enough. We make a assumption, create a bias, to simplify the amount to search.
#### Simple Rules
- We can guess a Hypothesis Class (a subset of the Hypothesis Space) and see if the model converges into the hypothesis/concept
	- (The following rules are very abstracted. In reality, we would use regression, decision trees, etc.)
- Conjunction ($\land$)
	- ![[Pasted image 20230105152653.png|400]]
 - $m$-of-$n$ rules (e.g. if 3 out of 4, then true)
	 - ![[Pasted image 20230105153053.png|400]]
		 - LETS GO, WOW we found the hypothesis at the \*\*\*, right?
## 2 Views of Learning
- ![[Pasted image 20230105153347.png|400]]
	- We can have multiple correct hypotheses, which muddies the water on which is actually the right function. Even then, those could still be wrong.
----
[Chapter 1 — Introduction to Machine Learning and Design of a Learning System | by Pralhad Teggi | DataDrivenInvestor (medium.com)](https://medium.com/datadriveninvestor/3-steps-introduction-to-machine-learning-and-design-of-a-learning-system-bd12b65aa50c)
# Simple Learning Process
- ![[Pasted image 20230118130211.png|400]]
## Hypothesis
- A hypothesis is a function that best describes the target and Hypothesis set or space H(.) is the collection of all the possible legal hypothesis.
	- The goal of the learning process is to find the final hypothesis that best approximates the unknown target function. (aka the target concept)
- Our hypothesis is an Inductive Learning Hypothesis
	- which is learning based on experience (On the basis of past experience, formulating a generalized concept.)
# Enjoy Sport
- possible training data
	- ![[Pasted image 20230118132542.png|300]]![[Pasted image 20230118132741.png|170]]
- hypothesis
	- ![[Pasted image 20230118132641.png|250]]
- The Learning Task
	- ![[Pasted image 20230118132722.png|300]]
		- Conjunction means h(x) & c(x) = Sky $\land$ AirTemp $\land$ Huidity $\land$ Wind $\land$ Water $\land$ Forecast
- Hypothesis Space
	- Syntactically Distinct
		- ![[Pasted image 20230118132741.png|200]]
			- X = count of each attributes multiplied
			- H = count of each attributes + 2 (for ? & 0) multiplied
	- Semantically Distinct
		- These 2 hypothesis is semantically similar because both none of these will except a day (one attribute is never accepted)
			- ![[Pasted image 20230118132811.png|200]]
			- ![[Pasted image 20230118134022.png|250]]
- General vs Specific Hypothesis
	- h1 = (Sunny, ?, ?, Strong, ?, ?) and h2 = (Sunny, ?, ?, ?, ?, ?)
		- h2 is more general, having fewer constraints
			- $h_{2} \geq_{g} h_{1}$
		- any instance classified positive by h1 will also be classified positive by h2, so .
	- Another ex
		- ![[Pasted image 20230118134625.png|400]]
- FIND-S: an algorithm to find attempt to find a target concept for EnjoySport
	- ![[Pasted image 20230118134929.png|400]]
	- Execution:
		- ![[Pasted image 20230118135117.png|400]]
			- Start with most specific hypothesis (all 0s)
			- + are positive examples (output = Yes) that got accepted and made hypothesis more general
			- - (output = No) are ignored
			- FIND-S found that h(x) = **<Sunny, Warm, ?, Strong, ?, ?>**
	- Questions
		- Is this correct?
		- Why prefer to start at most specific?
		- What if the training data have outliers?
		- Multiple correct hypotheses?
- Consistent and Version Space
	- A Consistent Hypothesis
		- A hypothesis h is consistent with a set of training examples D if and only if h(x) = c(x) for each example (x, c(x)) in D 
			- (aka the hypothesis/target concept and the training examples match outputs).
			- ![[Pasted image 20230118135719.png|300]]
				- This is different from a satisfying hypothesis, which requires h(x) = 1
			- FIND-S's h(x) = **<Sunny, Warm, ?, Strong, ?, ?>** is consistent to the training examples D
	- Version Space
		- The version space, with respect to hypothesis space H and training examples D, is the subset of hypotheses from H consistent with the training examples in D
			- (aka the subset of hypotheses that are consistent)
			- ![[Pasted image 20230118140343.png|200]]
- The List-Then-Eliminate algorithm
	- A way to represent the version space
	- ![[Pasted image 20230118140936.png|300]]
		- Kinda like exhaustive search. You have to list all possible hypotheses before eliminating to find the right one. So only finite and feasible amount
	- EnjoySport's Version Space
		- ![[Pasted image 20230118141557.png|400]]
			- h1 & h2 are most generic (generic boundary, G) and h6 is most specific (specific boundary, S)
- Version Space representation theorem
	- ![[Pasted image 20230118143126.png|350]]
		- ![[Pasted image 20230118143058.png|300]]
	- EnjoySport's Version Space
		- ![[Pasted image 20230118143517.png|400]]![[Pasted image 20230118143651.png|300]]
			- Kinda like interpolating through S & G
- Candidate Elimination Algorithm
	- Computes the Version Space
		1. Set Generic Boundary $G_0$ = { \<?, ?, ?, ?, ?, ?\> } & Specific Boundary $S_0$ = { \<0, 0, 0, 0, 0, 0\> }
			- This will include the whole hypothesis space, H.
		2. Repeat train all data, result is the bounds of hypotheses that are all consistent. (Pic below shows what to do for + & - examples, it's similar to FIND-S but also accounts for negative)
			- ![[Pasted image 20230118144358.png|400]]
	- EnjoySport & Candidate Elimination
		- ![[Pasted image 20230118144906.png|300]]
		- Init:
			- $G_0$ = { \<?, ?, ?, ?, ?, ?\> } & $S_0$ = { \<0, 0, 0, 0, 0, 0\> }
		- Example 1: (+)
			- $G_1$ = { \<?, ?, ?, ?, ?, ?\> } & $S_1$ = { \<Sunny, Warm, Normal, Strong, Warm, Same\> }
				- Nothing in G to remove
				- S was too specific
		- Example 2: (+)
			- $G_2$ = { \<?, ?, ?, ?, ?, ?\> } & $S_2$ = { \<Sunny, Warm, ?, Strong, Warm, Same\> }
				- Nothing in G to remove
				- S was too specific
		- Example 3: (-)
			- $G_3$ = { \<Sunny, ?, ?, ?, ?, ?\>, \<?, Warm, ?, ?, ?, ?\>, \<?, ?, ?, ?, ?, Same\>} & $S_3$ = { \<Sunny, Warm, ?, Strong, Warm, Same\> }
				- Nothing in S to remove
				- G was too general
					- We used the $S_3$ = { \<Sunny, Warm, ?, Strong, Warm, Same\> } to 
		- Example 4: (+)
			- $G_4$ = { \<Rainy, Cold, High, Strong, Warm, Same\> } & $S_4$ = { \<Sunny, Warm, ?, Strong, ?, ?\> }
			- //TODO
# Inductive Bias
- Chart for Algorithms
	- ![[Pasted image 20230118170758.png|500]]
## Low vs High Inductive Bias
- The higher the bias, the more assumptions you have to reduce the hypotheses space, and vice versa
## A Biased Hypothesis Space
- Suppose the target concept c(x) is not contained in the hypothesis space H.
	- Then we don't have a consistent hypothesis