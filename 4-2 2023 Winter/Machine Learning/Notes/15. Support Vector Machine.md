# SVM Intro
- Uses activation functions/units (i.e. sigmoid, etc.), but instead of being content for a decision boundary found by gradient descent's local minimum, SVMs will find the decision boundary that maximizes the margin between classes.
	- ![[Pasted image 20230301170226.png|250]]
	- Extra computational cost to maximize that margin.
## Applications
- Predicts binary classification if a relationship of a couple will get better from therapy session speech features
	- Was good even with a relatively small example ~100
- Predicts binary classification if someone has mild vs severe Covid-19
	- Trained on clinical data of ~800 patients.
# What is a SVM?
- SVM needs 3 inputs
	- A subset of the training examples to be the support vectors.
	- A vector of weights for those support vectors.
	- A similarity function $K(x,x')$ called the kernel (no relation to OS, Matrices, or CNN).
- The class prediction for a new example $x_q$ is
	- ![[Pasted image 20230301171256.png|300]]   
	- We are focused on binary classification, so the output is
		- ![[Pasted image 20230301171308.png|200]]
	- The inputs are defined as
		- ![[Pasted image 20230301171536.png|200]]
			- we want to use $p$ because we want to differentiate that those are used as support vectors
	- Used of the support vectors has a weight
		- $\alpha_1$, $\alpha_2$,... $\alpha_p$,
	- So, $f(x_q)$ is the weighted sum of the similarity times the known instances.
		- Also, $\text{sign}(z)$ returns 1 if positive, -1 if negative or 0.
## Perceptron Revisited
- ![[Pasted image 20230301173213.png|400]]
	- A perceptron is very similar to SVM already, since both are trying to find a line for a decision boundary. SVM goes further by wanting to maximize the margin between that decision boundary line, creating better generalization.
- ![[Pasted image 20230301173343.png|400]]
	- We can choose the dot product of the support vectors for the similarity function, using the perceptron as a special case of weighted kNN. 
		- When you have similar points, the similarity will blow up
	- This allows for defining linear boundaries for SVMs.
# Explanation of Kernels
- SVMs can be thought of as a generalization of the perceptron where the dot product is replaced by a similarity function, called a kernel.
	- ![[Pasted image 20230301174035.png|400]]
	- ![[Pasted image 20230301173849.png|400]]
		- Convex weight optimization problems only have one optimum points, which is unlike gradient descent in CNN.
## Examples of Kernels
- ![[Pasted image 20230301174511.png|225]]
	- Polynomial: raise the dot product to $d$
- ![[Pasted image 20230301174455.png|400]]
	- Gaussian: aka Radial Basis Function, finds the Euclidean Distance for similarity.
### Polynomial Kernel
- Let's see an example on finding the similarity / evaluating the kernel for two 2D vectors $u$ & $v$
	- ![[Pasted image 20230301174801.png|90]]
	- ![[Pasted image 20230301174913.png|400]]
		- As you can see on the right hand, the quadratic is rewritten into a dot product of 3D vectors.
- Linear kernels can't represent quadratic frontiers, unlike polynomial kernels.
	- ![[Pasted image 20230301175332.png|300]]
		- The 1st is working, you can separate those 2. 
			- The black line is the boundary
			- The thin lines are the frontiers
			- The circled dots are the support vectors
		- The 2nd is not., we need more than one line
		- That's why in 3, you can see that squaring those 1D dot product allowed use to move into 2D, allowing for a correct boundary.
	- Same thing for 2D input vectors, make a 3D parabolic thing, and slice it with a plane.
# Learning SVMs
## Linearly Separable Case
- Let's start ez. We have a linearly separable case.
	- ![[Pasted image 20230302141038.png|200]]
	- Which line should we choose for the boundary, and how?
- Summary
	- ![[Pasted image 20230302141435.png|300]]

### Primal Problem
- Given: training ex. $(\vec{x_i},y_{i})$ where $i=1,...,N$
- Goal: Decision Boundary: $\vec{w} \cdot \vec{x} + b = 0$
	- $b$ = bias
- Distance between boundary and $(\vec{x_i},y_{i})$ is:
	- $|\vec{w} \cdot \vec{x_{i}}+b| / ||\vec{w}||$ = $y_{i}(\vec{w} \cdot \vec{x_{i}}+b) / ||\vec{w}||$
		- $||\vec{w}|| = \sqrt{\vec{w} \cdot \vec{w}}$
- We need to find $\vec{w}$ and $b$ that maximize
	- $\min^{N}_{i=1} \frac{y_{i}(\vec{w} \cdot \vec{x_{i}}+b)}{||\vec{w}||}$ = $\frac{2}{||\vec{w}||} \min^{N}_{i=1} y_{i}(\vec{w} \cdot \vec{x_{i}}+b)$
- Assume $\vec{w}$ and $b$ are scaled such that for closest training example  $y_{i}(\vec{w} \cdot \vec{x_{i}}+b) = 1$
	- Hence  $y_{i}(\vec{w} \cdot \vec{x_{i}}+b) \geq 1$ for $i=1,...,N$
- Primal Problem
	- Find $\vec{w}$ and $b$ that minimizes  $\sqrt{\vec{w} \cdot \vec{w}} / 2$ subject to $y_{i}(\vec{w} \cdot \vec{x_{i}}+b) \geq 1$ for $i=1,...,N$
### Dual Problem
- Find LaGrange Multipleis $\alpha_{i} \geq 0$ $(i=1,...,N)$ that maximizes $\min\limits_{\vec{w},b} (\frac{1}{2}\vec{w} \cdot \vec{w}-\sum\limits^{N}_{i=1} \alpha_{i} y_{i}(\vec{w} \cdot \vec{x_{i}}+b) - 1)$
	- The Sum is a augmented directive function $L(\vec{w}, b, \vec{\alpha})$
	- The partial derivative $\frac{\partial L}{\partial \vec{w}}(\vec{w}, b, \vec{\alpha})$ = aids
- So the simplified Dual Problem is
	- Find $\alpha_{i}\geq 0$ $(i=1,...,N)$ that maximizes
	- $\sum\limits_{i} \alpha_{i} - \frac{1}{2} \sum\limits_{i} \sum\limits_{j} \alpha_{i} \alpha_{j} y_{i} y_{j} \vec{x_{i}} \cdot \vec{x_{j}}$ subject to $\sum\limits_{i} \alpha_{i} y_{i} = 0$
### Exercise
- ![[Pasted image 20230302150431.png|150]]
	- Let the dots be $y=-1$, and the triangle be $y=1$ (binary classification)
- We need to find all $\alpha$ that maximizes the Simplified Dual Problem
	- This will make you find the dot products and you will get a maximization problem to plug into wolfram alpha because it's too hard.
	- $(\alpha_{1}, \alpha_{2}, \alpha_{3})$ = $(\frac{2}{5}, 0, \frac{2}{5})$ meaning that the support vectors are $\vec{x_{1}}$ and $\vec{x_{2}}$
	- Now we have to find the weight vector $\vec{w}= \sum\limits^{N}_{i=1} \alpha_{i} y_{i} \vec{x_{i}}$
		- which $\vec{w_{1}}=\frac{2}{5}$ and $\vec{w_{3}}=\frac{4}{5}$
	- The bias is found by algebra
		-  $[(\text{weights}) \cdot (y \text{ values TODO}) + b]$ = $[(\frac{2}{5}, \frac{2}{5}) \cdot (1,1) + b]$= $-1$
		- so $b$ = $-1-(6/5)$ = $-11/5$
- ![[Pasted image 20230302152302.png|150]]
- 
## Non-Linear Case
- Let's see a 1D example.
	- ![[Pasted image 20230307141832.png|300]]
	- You can't split the examples with a linear boundary (not linearly separable in 1D).
- We must change the kernel. Instead of just a dot product, square it, making a quadratic kernel.
	- ![[Pasted image 20230307143721.png|500]]
	- (If this doesn't separate it, cube it. Still no? Then ^4 it.)
- So now the training becomes
	- ![[Pasted image 20230307143913.png|400]]
	- ![[Pasted image 20230307144205.png|400]]
		- 16 terms, wolfram alpha clutch
			- "maximize a+b+c+d-(1/2)(a^2)-2(b^2)-12.5(c^2)-50(d^2)+(a\*b)-(a\*c)+9(b\*c)+16(b\*d)-49(c\*d) subject to a-b+c+d=0, a >= 0, b >= 0, c >= 0, d >= 0"
			- (This is where the computation gets heavy. The more examples you have, the more $\alpha$ you have to optimize.)
		- results = (4,6,2,0)
			- ![[Pasted image 20230307145124.png|200]]
			- Because $\alpha_{4}$ = 0, it is not a support vector
- Now we must solve the inference boundary
	- ![[Pasted image 20230307145158.png|450]]
	- ![[Pasted image 20230307145440.png|200]]
- Ez
	- ![[Pasted image 20230307145423.png|300]]
## Handling Noise
- ![[Pasted image 20230307145732.png|200]]
- Solution: Allow some points to be within the margins.
	- ![[Pasted image 20230307145830.png|200]] ![[Pasted image 20230307150217.png|200]]
	- This is done by
		- ![[Pasted image 20230307145846.png|250]]
		- The slack variable is the length of how far from the margins the point is.
		- We want to minimize slack, and $C$ is a hyperparameter to change the strength of the minimization.
			- When $C=0$, there is no slack minimization, you don't care if the the point is all the way across the map.
			- When $C$ is too big, you will be overfitting as the slack will move the margins too much.
## Multiclass Classification
- A typical way: 1-vs-All
	- e.g. w/ 4 classes, ensemble 4 SVMs
		- 1 classifying attribute 1 vs 2,3,4
		- 1 classifying attribute 2 vs 1,3,4
		- 1 classifying attribute 3 vs 1,2,4
		- 1 classifying attribute 4 vs 1,2,3