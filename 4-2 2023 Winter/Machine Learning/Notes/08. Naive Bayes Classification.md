# Naive Bayes Classifier
- A table of conditional probabilities that are multiplied at classification time. 
# Naive Bayes Assumption
- ![[Pasted image 20230201152134.png|400]]
	- Example has 4 attributes (3 binary, 1 ternary)
	- We want to guess the class (mammal vs non-mammal)
		- If we can get the probabilities 
			- ![[Pasted image 20230201152213.png|300]] (is mammal)
			- ![[Pasted image 20230201152249.png|300]] (non-mammal)
			- We can guess which classification is more probable
		- Formally
			- ![[Pasted image 20230201152847.png|300]]
			- ![[Pasted image 20230201153429.png|250]]
				- $v_{MAP}$ is the what is picked for the classification.
				- Look at all the conditional probability of a class label given the input's attributes
					- argmax is saying we want to return the V (output variable) where it produces the max probability from all
					- This is impractical, e.g., with $n$ binary attributes, it needs $2^{n+1}$ probabilities calculations (scalability issue)
				- So we can use Bayes Rules to reduce the probabilities we need to calculate
					- ![[Pasted image 20230201153549.png|300]]
				- And we can drop the denotator because it isn't dependent with argmax
					- ![[Pasted image 20230201153705.png|200]]
					- This also means that the output of this is no longer a true probability, as we dropped the part that normalizes. (It's like using non-sqrt() Pythagorean thm as a measure)
						- So don't call it  $v_{MAP}$, but $v_{NB}$.
			- Furthermore, apply the **Naive Bayes Assumption** (bias)
				- Input attributes are conditionally independent given the class label 
					- (e.g. the probability of being able to fly is not affected by living in the water, and any other attributes)
					- ![[Pasted image 20230201153902.png|200]]
			- Which gives us
				- ![[Pasted image 20230201153924.png|200]]
				- From exponential to now only linear.
					- ![[Pasted image 20230201154056.png|200]] (in our ex: $4n+2$)
## Is the Assumption met?
- Naive Bayes Assumption means
	- ![[Pasted image 20230201172412.png|200]]
		- Independent
- Something like this doesn't satisfy the assumption
	- ![[Pasted image 20230201172438.png|200]]
	- ![[Pasted image 20230201172532.png|300]] (not equals above)
# Psuedocode
- ![[Pasted image 20230201155332.png|300]]
	- Training is just counting & estimating probability then store into table
	- Inference is then reading those values to do probability finding
# Mammals vs Non-Mammals Walkthrough
- ![[Pasted image 20230201152134.png|400]]
- Let's guess the new unknown animal given at the bottom
	- First, let's find the "probability" for $v_1$ (mammal)
		- P(mammal) = P(being a mammal)\*P(a mammal that can give birth)\*P(a mammal that can't fly)\*P(a mammal that lives in the water)\*P(a mammal that has legs)
		- ![[Pasted image 20230201160221.png|200]]![[Pasted image 20230201160338.png|200]]
	- Then, for $v_2$ (non-mammal)
		- P(non-mammal) = P(being a non-mammal)\*P(a non-mammal that can give birth)\*P(a non-mammal that can't fly)\*P(a non-mammal that lives in the water)\*P(a non-mammal that has legs)
		- ![[Pasted image 20230201160517.png|200]]![[Pasted image 20230201160527.png|200]]
	- Since mammal has a higher "probability", then 
# Naive Bayes Subtleties: Underflow
- Floating point underflow can happen when you multiply too many probabilities that's close to 0.
- Solution: log the probability finding
	- ![[Pasted image 20230201162209.png|300]]
	- And because log is increasing and
		- ![[Pasted image 20230201162313.png|200]]
	- Our probability can be rewritten to addition, avoiding multiplication underflow
		- ![[Pasted image 20230201162328.png|300]]
# Naive Bayes Subtleties: Target Class/Value don't have example for an Attribute
- What if we have a mammal that walks in which sometimes has legs?
	- This will throw off the probability calculation as that attribute's probability is 0
		- ![[Pasted image 20230201162720.png|150]]
- Solution: m-estimate / Laplace Smoothing
	- Replace the conditional probability with:
		- ![[Pasted image 20230201163032.png|150]]
		- ![[Pasted image 20230201163111.png|250]]
			- "$n_c$ out of $n$"
				- ![[Pasted image 20230201163207.png|100]]
			- $m$ = possible attributes value
			- 
- Ex: Estimating Mammals with "Lives in Water" attribute
	- Uh oh, no mammal sometimes lives in water
		- ![[Pasted image 20230201163618.png|200]]
	- Let $m$ = 3 & $p$ = 1
		- This is saying that we (surely) seen 3 mammals in the past that has 1 as yes, 1 as no, and 1 as sometimes
			- It then gives all 3 attributes values the same weight will avoiding probability of 0
		- So the probabilities are now
			- ![[Pasted image 20230201163910.png|200]]
		- It doesn't have to be like this, we can choose a $p$ for each ($p \leq m$) to create different weights
# Naive Bayes and Text Classification
## Train
- Training Data
	- 700 texts by males
	- 300 texts by females
- Prior Probability (for Laplace Smoothing)
	- P(m) = 0.7
	- P(f) = 0.3
- Create Vocab
	- ![[Pasted image 20230201164631.png|125]]
	- sklearn CountVectorizer be like
- For Every word $w_i$ in Vocab $W$
	- Compute the conditional probabilities of the word given each gender
		- ![[Pasted image 20230201164742.png|125]]
			- aka for male: The conditional probability of seeing the word $W=w_i$ for all the words written by a male author $C=m$
- If there's a word that been seen in one but not the other gender, use Laplace Smoothing
	- ![[Pasted image 20230201170001.png|200]] 
		- Always 1 and over Vocab $W$ count
## Deploy
- Break new input text into list of words
	- ![[Pasted image 20230201171532.png|80]]
- Then compute the assumed conditional probabilities for each new word $a_i$ and for both male and female 
	- ![[Pasted image 20230201171730.png|300]]
- Pick the higher assumed probability of the above calculations