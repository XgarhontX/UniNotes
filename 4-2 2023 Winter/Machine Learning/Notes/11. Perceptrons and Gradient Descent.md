# Gradient Descent
- The greedy method to find the minimum of a function $f$
- by taking steps proportional to the negative of the derivative $fâ€™$
	- ![[Pasted image 20230210175817.png|100]]
		- $\eta$ is the learning rate (aka how much you move per check)
	- (aka, with a starting point, move left if the derivative is +, move right if the derivative is - to find the bottom of the valley)
- That means $f$ must be differentiable
- Greedy, can get stuck at local minimum with no guarantee to finding the global
	- You can try restart the method from different point of the function, hopefully different enough that it isn't the same local minimum
	- $\eta$ can be dynamic, a good choice is one that balances finding the minimum while not be too accurate and slow
		- Start with high value, then become smaller as training proceeds
- Used heavily in neural network training
	- Use gradient descent with weights of the model
	- Because of random gradient descent starting point, a model with the same data can train and be different
- "Gradient" is because we are looking at derivatives of higher dimensions
	- ![[Pasted image 20230210180729.png|220]]![[Pasted image 20230210180840.png|120]]     ![[Pasted image 20230210180826.png|120]]
		- $f$ here is dependent on multiple variables $x, y$
## Example Calculation
- Apply gradient descent to find the minimum of
	- ![[Pasted image 20230210181004.png|150]]
1. Find partial derivatives for each dependent variables
	- ![[Pasted image 20230210181730.png|50]] = $2x-2$
	- ![[Pasted image 20230210181748.png|50]] = $6y$
	- So the gradient is
		- ![[Pasted image 20230210181813.png|150]]
2. let $\eta = 0.1$ and start at $(1,10)$
	- So let's find the next point
		- ![[Pasted image 20230210182107.png|125]] --> $x \leftarrow x - \eta (2x-2)$ = $x \leftarrow 1 - (0.1) (21-2)$ = $1$
		- ![[Pasted image 20230210182252.png|125]] --> $y \leftarrow y - \eta (6y)$ = $y \leftarrow 10 - (0.1) (6*10)$ = $4$
	- After one move, we went to $(1,4)$. Hopefully this is to the global minimum
3. Repeat 2
	- Next is $(1,1.6)$
	- After more blah blah more moves, you should be close to $(1,0)$, which is the actual minimum of the function
# Neural Network Applications
## 1989 Elon Ma Tesla Autodrive AI!! :shocked:
- ![[Pasted image 20230210183111.png|200]]
	- 30 Output Units, for 30 possible steering wheel angles
		- Which ever has the highest probability calculated will be the steering angle outputted
	- 4 Hidden Units
		- More units, better backpropagation
	- Training Data
		- X is the camera footage of someone driving
		- y is the steering wheel angle of the person driving
## 1988 NETTalk Text to Speech
- ![[Pasted image 20230210183818.png|300]]
	- Input layer is the text characters
		- 7 \* 29 inputs is saying there is windows of 7 characters, where each window is shifted by one next character and can contain 29 possible character (alphabet and some punctuations)
		- Characters are represented by One Hot Encoding (i.e. a bit string of length 29)
			- ![[Pasted image 20230210184619.png|150]]
	- 80 hidden units
	- 26 output units to create the sound
	- This is an example of a Multilayer Perceptron (because of the sizeable hidden middle layer), which was commonly implemented back then
# Perceptron
- Equivalent of a biological neuron for neuron network
- ![[Pasted image 20230210185239.png|400]]
	- Take in $n$ numeric input signals $x$, each weighted $w_n$ 
		- e.g. gender inference from text where an input signal is the number of times "wow" is used by a male author
		- There's also this fudge factor / dummy input $x_0=1$ with weight $w_0$
	- Then sum up all weighted input signals
	- Then into a step function 
	- The output is 1 if the weighted sum after the step function > 0, -1 if otherwise
	- The blue is the same thing, 
		- ![[Pasted image 20230210190545.png|200]]
		- but we want to use $w_{0},x_0$ as a dummy input to simplify it to 
			- ![[Pasted image 20230210190727.png|300]]
				- ($w_{0} \cdot x_0$ is dot product)
		- The significance of the dot product means the decision boundary is linear
			- n=2 means line
			- n=3 means plane
## Perceptron Representing $\land$
- Let's say we have two inputs $x_1$ & $x_2$
	-  ![[Pasted image 20230210191317.png|125]]![[Pasted image 20230210191238.png|150]]
- A Perceptron creates a linear boundary, which is enough to classify $\land$
	- ![[Pasted image 20230210192053.png|325]]
		- To get the weights, we drew a line and picked weights to make outputs match the line. This is where $w_0$ fudge factor comes in, allow us to move the line
## Decision Surface of a Perceptron
- However in reality, the weights are randomly picked. Then through showing a new training example, the line is moved to fit the training example. 
	- ![[Pasted image 20230210192741.png|120]]
- If the data is isn't linearly separable, we need a network of perceptron.
	- ![[Pasted image 20230210192712.png|100]] (XOR)
# Perceptron Training Rule
1. Initially assign small (-1 to 1) random values as weights
2. Show examples from data set one by one ($\overrightarrow x$)
3. Update weights
- ![[Pasted image 20230210195333.png|100]]    ![[Pasted image 20230210195343.png|100]]
	- ![[Pasted image 20230210195502.png|250]]
		- $o$ is compared to $t$ by the subtraction in $\Delta w_i$ 
			- if both are equal, there is no weight difference to adjust/update
			- otherwise, this difference gets multiplied by the learning rate $\eta$ and input $x_i$
## Pseudocode
- ![[Pasted image 20230210195944.png|250]]
	- sign() makes negative numbers = -1, and positive = 1
# Delta Rule
- Perceptron Training Rule is proven to converge if the data is linearly separable and when the learning rate is small enough.
	- If those 2 conditions are not met, there will be a point where the weights will start oscillating wildly when given new examples.
- Delta Rule, however, will allow the model to converge on a hypothesis even if the data isn't linearly separable.
	- Converge: weights will no longer change ("found the line", "found global min", "found hypothesis")
	- Applied to a linear unit, a sigmoid unit, MLP, and also deep neural networks
## Linear Unit
- ![[Pasted image 20230210201003.png|300]]
	- With this, we need the weights that minimizes the squared error (aka Loss)
		- ![[Pasted image 20230210201424.png|350]]
			- $d$ is the current training example
			- $t_d$ is the current target label
		- Error Function dependent on 2 weights
			- ![[Pasted image 20230210201532.png|300]]
				- Having the SQUARED Error makes it differential everywhere
	- The update rule:
		- For 2 weights
			- ![[Pasted image 20230210201641.png|180]]          ![[Pasted image 20230210201651.png|180]]
		- General
			- ![[Pasted image 20230210201729.png|200]]
				- We can simply the fraction part
					- ![[Pasted image 20230210201850.png|200]]
					- ![[Pasted image 20230210201922.png|125]]
			- ![[Pasted image 20230210202247.png|300]]
## Pseudocode
- ![[Pasted image 20230210202359.png|300]]
	- A good termination condition is when the error/loss no longer improves
	- aka Standard Gradient Descent
	- Extra: Stochastic Gradient Descent
		- Do batch of examples, then update weights
		- Similar results, and much faster