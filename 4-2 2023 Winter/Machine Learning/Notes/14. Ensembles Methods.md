# Successful Applications of Ensembles
- Netflix Prize: 2006-2009 5 star rating predictions
	- Winning team had to use ensemble of 800 algorithms
		- The algorithms focused on different features
- Zillow Prize: 2017-2019 predicting prices of homes 
	- Take avg is better
# Better predictions through diversity
- A committee of models is better than just 1
- Protect against groupthink, each model should be independent (Heterogenous Ensemble)
- A large ensemble is better, even if some models barely pass the baseline
# Why/when do ensembles work well?
- Example: 5 Binary classifiers
	- Each 70% accurate
	- This is all the possible outcomes
		- ![[Pasted image 20230223135850.png|300]]
			- For 1/2 of the outcomes, the ensemble is correct
			- So what is the probability to get the correct ensemble prediction?
				- 1) $(0.7)^5$
				- 2-6) $(0.7)^4 * 0.3$
				- 7-16) $(0.7)^3 * (0.3)^2$
				- $P_{correct}$ = Sum of those 16 probabilities = $0.837$
					- Woah, it's higher than 0.7
		
- Example: 21 binary classifiers
	- Each 70% accurate
	 - ![[Pasted image 20230223140945.png|450]]
- Example: 101 Binary classifiers
	- Each 70% accurate
	- $\sum\limits_{j=51}^{101}(j \text{ out of 101 are wrong})*(0.7)^{j}*(0.3)^{101-j}$ = $0.9999999$
	- Geez, but in practice, it's hard to keep a high number of classifier to be independent
- Decision Tree Binary Classification Example
	- ![[Pasted image 20230223141355.png|400]]
		- Left are 3 different decision trees
			- Each 5 internal nodes
		- On Right, this is the boundary listening to the majority
			- 13 internal nodes, much more closer to the real boundary
		- (Extra) You want to do 
# Bagging
- This is a Homogeneous Ensembles
- aka Bootstrap Aggregations
- ![[Pasted image 20230223142214.png|400]]
	- Replicates of datasets is done by creating bootstrap replicates w/ replacement
		- This means the replicated set randomly picks out the same amount of elements
		- If Alice is in a $n$ size dataset
			- prob to select Alice = $1/n$
			- prob to not select Alice = $1 - 1/n$
			- prob to not pick Alice in $n$ attempts = $(1-1/n)^n$
			- prob of selecting Alive at least once in $n$ attempts = $1 - (1-1/n)^n$
				- Take $\lim_{n \rightarrow inf}$ = 0.632
		- The goal is to create diversity in the data, and maintain the same model algorithm
	- Then train UNSTABLE models (non deterministic, i.e. Decision Tree ID3) for each 
	- Training each model is parallel and independent
## Random Forest
- Random Forest is Bagging with Decision Trees + Subspace Sampling
	- ![[Pasted image 20230223143325.png|400]]
	- Subspace Sampling is the random aspect
		- In the picture, it is with the bagging only pick pairs of attributes from the 3 total for each dataset
- Random Forest can be evaluated by using the data that was omitted by Subspace Sampling
- Random Forest is fine to train up to full depth, there is no overfitting as the multiple Decision Trees mitigate it's affect
	- ![[Pasted image 20230223143721.png|150]]
- sklearn has RandomForest
	- n_estimators is the hyperparameter for dataset/Trees
		- Don't have more trees than the data size itself, or you go boom and get 0 accuracy because you're training on 1 example per tree
# Boosting
- This is a Homogeneous Ensembles
- This trains the model in sequence, unlike Bagging, and is to correct the prior model's shortcomings.
- Deterministic with random example ordering
## AdaBoost (Adaptive Boosting)
- Shortcomings are identified by high-weight data points
- ![[Pasted image 20230223145201.png|250]] ![[Pasted image 20230223145256.png|300]]
	- Usually, the models are Decision Stump (tree ends immediate from root)
		- The weight vector (vector is for each feature/dimension) is increased when the instance is misclassified. This is what keeps the history of failed classification, which will let the future model prioritize it.
### How to learn from weighted training examples?
- ![[Pasted image 20230223145754.png|350]]
	- The sampling is to copy only increasing
		- ![[Pasted image 20230223150132.png|300]]
### Pseudocode
- ![[Pasted image 20230223150155.png|300]]
	- Learner is the model that can take in weights
	- The sum of weights of incorrectly classified instances at the end of the for loop after weights are adjusted = 0.5
		- The start's sum = $\epsilon_r$
		- This redistributes the weights so that the model does as bad as random guessing.
### PlayTennis
- ![[Pasted image 20230223150940.png|400]]
	- Weights that increase means that the classifier misclassified, and vice versa
	- $\epsilon_r$ = $4/14$
		- $w_i$ of correct = (1st top right formula) = $1/20$ 
		- $w_i$ of incorrect = (2nd top right formula) = $1/8$
### Confidence Score
- The confidence factor is calculated at the end of the pseudocode
	- ![[Pasted image 20230223151836.png|150]]
	- - ![[Pasted image 20230223151515.png|200]]
- Example:
	- ![[Pasted image 20230223151525.png|200]]
		- the confidence factor is used for weighted voting
			- the vote is multiplied with the weight
			- The vote here is No with a confidence of 2.1 vs 1.3
## Gradient-Boosted Decision Trees (GBDT)
- Shortcomings are identified by gradients
- ![[Pasted image 20230223151855.png|300]]
	- ![[Pasted image 20230223152122.png|150]]
	- Regression Tree
		- ![[Pasted image 20230223152108.png|150]]
### Possible Situation
- ![[Pasted image 20230223152151.png|325]]
	- We can create a model to predict the mistake of your friends model. This is calculating the Residuals.
### In general 
- ![[Pasted image 20230223152357.png|320]]
- ![[Pasted image 20230223152434.png|320]]
### Pseudocode
- ![[Pasted image 20230223152544.png|400]]
	- e.g. We start with the avg baseline model in the project.
		- Then train a model to predict the error of the prior
		- With both values, train a new model to predict actual values
		- Repeat
	- $l$ is the max iterations
- Not parallels, can be slow.