# Area Under the Curve
- evaluationmeasures.pdf in week3
- for midterm
- When you have an unbalanced data set (e.g. 99% is positive examples to 1% negative), accuracy is not a good measure
## Confusion Matrix
- ![[Pasted image 20230202142051.png|200]]
	- TP = True Positive, TN = True Negative
	- FP = False Positive, FN = False Negative
	- True Positive Rate: TPR = TP / (TP+FN)
		- (best is = 1)
	- False Positive Rate = FPR = FP / (FP+TN) 
		- (best is = 0)
## ROC-curve
- ![[Pasted image 20230202142805.png|300]]
	- If we take a the area under the green line, it's 0.5, which is the average accuracy
	- By setting up thresholds to classify positive vs negative (TODO how?) in our data, we can get the TPR & FPR and create a ROC-curve
		- ![[Pasted image 20230202143148.png|300]]
			- This is the better accuracy measure. We can strive to increase the area under the curve to achieve better accuracy
- If we don't have a binary classifier
	- Quantize the data to create threshold in classifications
# Logistic Regression
- for final
- A mini neural network type beat
	- Will spit out a number between 0 & 1 (non-inclusive), as a probability for classification
- ![[Pasted image 20230202145257.png|500]]
	- ![[Pasted image 20230202145531.png|300]] ![[Pasted image 20230202145600.png|200]]
		- $x_i$ must be numerical
			- you can give numbers to attributes with one-hot-encoding (e.g. $x_1$$x_2$$x_3$: 001, 010, 100 for 3 values)
		- The output is through SIGMOID to limit between 0 & 1 (non-inclusive)
# Differential Privacy
- ![[Pasted image 20230202150152.png|400]]
- ID3 we did is deterministic, never changing when training on the same data set
	- Models especially like that can leak information about the data set (like how similar and what not, or even reversed engineer to obtain the training data set)
- We can implement some variation/randomness/noise to the training process to prevent leaks, create differential privacy.
	- there is a measure (e-differential) to tell if some features in adjacent datasets don't get memorized.
	- The tradeoff is accuracy. 
- Research is ongoing to check if a model is guaranteeing differential privacy
# Final
- Ch
	- 1
	- 2
	- 3
	- Bayes
		- 6.1
		- 6.2
	- NB classifier
		- 6.9
		- 6.10
	- kNN
		- 8.1 
		- 8.2