# Multilayer Networks of Sigmoid Units
- A perceptron only works on linear boundaries.
- By stacking perceptrons, we can represent non-linear boundaries.
- ![[Pasted image 20230215165915.png|200]] ![[Pasted image 20230215165929.png|300]]
	- This is an ex of a multilayer network, able to predict a word from attributes F1 & F2.
	- There is a hidden layer of 7 sigmoid units layered before an output layer of more sigmoid units
		- "Input -> Hidden -> Output" is the standard stack
		- These layers are typically called a fully connected (FC) layers or dense layers because each node on a layer connect to all other nodes in adjacent layers
	- The decision boundary is way more complex.
# Sigmoid Unit
- ![[Pasted image 20230215170513.png|350]]
	- Just like a perceptron, but instead of a step function, we use the sigmoid function, which clamps the values between 0 and 1 but also allows it to be any values between it.
	- Being continuous and having a differentiable slope for all $z$ allows use to use gradient descent
## Training a Sigmoid Unit
- ![[Pasted image 20230215170928.png|350]]
	- We must FIND THE WEIGHTS that minimizes the Squared Error
- For the Error in the Update Rule, we can use a property for the sigmoid function's derivative
	- ![[Pasted image 20230215171536.png|150]], and here is the proof for it
		- ![[Pasted image 20230215175844.png|350]]
- So now we can take the partial derivative of the Update Rule
	- ![[Pasted image 20230215180250.png|500]]
		- We picked 1/2 in the Error just for this process here, since it cancels out the chain rule's 2
## Gradient Descent for Sigmoid Units
- ![[Pasted image 20230215180447.png|300]]
# Other Activation Functions
- Sigmoid unit: a neuron where the activation function is the sigmoid function, as covered in the video above.
- Tanh unit: a neuron where the activation function is the hyperbolic tangent (tanh) function instead of the sigmoid function. Notice the similarities in shape between the sigmoid function and the tanh function.
- A ReLU unit (rectified linear unit): a neuron where the activation function is the rectified linear function ReLU as depicted below. ReLU units are very popular in deep neural networks. This kind of activation function is much cheaper to compute than a sigmoid or tanh function, which matters in networks with 1000s of neurons or more.
- ![[Pasted image 20230215180658.png|450]]
	- ReLU returning 0 for $z\leq 0$ means it is willing to sacrifice some neurons.
		- A LeakyReLU prevents this by allow it to be slowly negative.
# Multilayer Perceptrons and Backpropagation
- Let's do the basic "In -> Hidden -> Out" Multilayer Perceptrons
	- $n$ input nodes ($x_{1}, x_{2},... x_{n}$ and $x_0=y_0=1$ as dummy for hidden layer)
	- $H$ hidden nodes ($y_{1}, y_{2},... y_{H}$ and $x_0=y_0=1$ as dummy for output layer) (sigmoid units)
	- $K$ output nodes ($o_{1}, o_{2},... o_{K}$) (sigmoid units)
	- Each node connects to all the adjacent layer's nodes (fully connected / dense layers)
	- ![[Pasted image 20230215183018.png|200]]
- We also need weights for each edge
	- ![[Pasted image 20230215183735.png|250]]
	- $w^{(2)}_{kh}$ is the weight for the edge connect $o_k$ and $y_h$ (hidden to output)
	- $w^{(1)}_{hi}$ is the weight for the edge connect $y_h$ and $x_i$ (input to hidden)
	- ![[Pasted image 20230215183805.png|200]]
- Steps
	- Compute all hidden layer nodes
		- ![[Pasted image 20230215184123.png|250]]
	- Compute all output layer nodes
		- ![[Pasted image 20230215184206.png|250]]
	- (HOLD UP) This is our Error/Loss Function
		- ![[Pasted image 20230215184358.png|200]]
		- But because doing both sums is aids, let's just do the sum for the output nodes (Stochastic Gradient Descent)
			- ![[Pasted image 20230215184726.png|150]]
	- Now we can find the update rule for the 2nd (hidden to output) layer
		- ![[Pasted image 20230215185217.png|450]]
	- And the update rule for the 1st (input to hidden) layer
		- ![[Pasted image 20230215190006.png|400]]
## Backpropagation Algorithm
- ![[Pasted image 20230215190449.png|400]]
	- First the input is used to calculate hidden and output.
	- Then the output calculates its error to update it's weights to the 2nd layer (hidden to output)
		- and also backpropagates by reporting it to the hidden layer 
		- so the hidden layer can calculate the its error and updates the weights on the 1st layer (input to hidden)
- ![[Pasted image 20230227142725.png|500]]
	- Weight Update: (Current Weight) + (Learning Rate)(Error of Out Node)(Input from In Node)
## Momentum
- ![[Pasted image 20230227142440.png|500]]
# Logistic Regression
- [Logistic Regression — ML Glossary documentation (ml-cheatsheet.readthedocs.io)](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#logistic-regression)
- Training a sigmoid unit to minimize the cross-entropy loss
	- ![[Pasted image 20230215191405.png|500]]
	- ![[Pasted image 20230215191417.png|150]]  ![[Pasted image 20230215191427.png|270]] ![[Pasted image 20230215191508.png|200]]
	- This is for binary classification (link above also covers multiclass classification)
	- The sigmoid function is your probability
	- Represents a linear boundary as the boundary is a threshold in the sigmoid function
## But WHY the hell we using cross-entropy loss? (Deriving Cross-Entropy Loss)
- Let's say, we are have a binary classification task.
	- ![[Pasted image 20230215192610.png|400]]
	- ![[Pasted image 20230215192244.png|400]]
		- We wanted to find the maximum likelihood hypothesis, this is it.
	- So intuitively
		- ![[Pasted image 20230215192737.png|300]]
			- (This is saying: depending on the expected output)
		- ![[Pasted image 20230215192818.png|200]]
			- So the target value is 1
- [Loss Functions — ML Glossary documentation (ml-cheatsheet.readthedocs.io)](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#:~:text=Cross%2Dentropy%20loss%2C%20or%20log,diverges%20from%20the%20actual%20label.)
	- Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.
		- ![[Pasted image 20230215193211.png|300]]
			- The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong!
				- (Strongly penalizes wrong high confidence predictions)
## HOW to use cross-entropy loss for weight update rule
- Instead of Sumo of Squared Error, use Cross-Entropy Loss and we can calculate the Weight Update function
	- ![[Pasted image 20230215193823.png|400]]
		- Yea, do Gradient Descent with this wow
# Softmax
![[Pasted image 20230227203144.png|500]]
![[Pasted image 20230227203159.png|500]]