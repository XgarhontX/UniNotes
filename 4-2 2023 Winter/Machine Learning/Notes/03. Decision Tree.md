# Decision Tree Hypothesis Space
- They are really ez to read and interpret.
- No restriction bias. We need supplements like search bias
## Example: Will GF say yes to playing tennis? 
- ![[Pasted image 20230109201403.png|350]]
	- She will say NO (Sunny Outlook & High Humidity leads to NO. Other features where not used)
	- All these features are discrete (value is finite, i.e. 3 options for Outlook)
- Continuous Features 
	- ![[Pasted image 20230109201715.png|350]]
		- Choose route based on threshold of features
## Example: Breast Cancer Prognosis 
- ![[Pasted image 20230109201943.png|200]]
- Decision Rule
	- ![[Pasted image 20230109202000.png|300]]
- Decision Tree from Rule
	- ![[Pasted image 20230109202450.png|250]]
## Decision Tree from Linear Boundary
- Given something like
	- ![[Pasted image 20230109202610.png|100]] (Boundary is linear blue line dividing them)
		- How can you find threshold for $x_{1} \& x_{2}$
	- NO, it's aids, you would need an infinite leaves. You can try to "pixelate" the line, approximating, but a manageable tree has better alternative with other techniques we learn later on.
## ## Decision Tree from Axis-Parallel Rectangle Boundary
- ![[Pasted image 20230109203212.png|300]]
# Growing Decision Tree from Training Data
## Top-down induction of decision trees
- ID3 Algorithm (Divide and Conquer)
	1. Choose best attribute as root
	2. Split learning sample
	3. Proceed recursively until each object is correctly classified
- ![[Pasted image 20230109203905.png|500]]
	- Well, Overcast is the easiest, that's instantly true, end it with a true leaf. (the recursive base case)
		- The others need more consideration, recursively grow the sub tree.
### Pseudocode
- ![[Pasted image 20230109204056.png|250]]
	- But how do we find a good splitting attribute?
#### Occam's Razor
- Prefer the simplest hypothesis that fits the data. (So the smallest depth is most optimal)
	- So, we want to split the data / pick thresholds that minimizes the Weighted Impurity
		- we use Entropy as Impurity (so it's aka Weighted Entropy)
- General Diagram:
	- ![[Pasted image 20230109204709.png|300]]
		- $|S_{1}|/|S|$ is the **count of data/examples of subset 1** divided by **the total data/examples count**
##### Example of Entropy Calculation (Bag of Candy Example)
- ![[Pasted image 20230109205012.png|300]]
	- p is for probability
	- Optimal Encoding for $B_2$
		- ![[Pasted image 20230109205211.png|300]]
	- Entropy of B
		- ![[Pasted image 20230109205314.png|200]]
		- $E(B_2$) = 2 (so 2 bits)
			- ![[Pasted image 20230109205408.png|300]] 
				- (she switched subscripts to numbers)
		- $E(B_1) = 0$ (so 0 bits, because it's always red)
			- $0*\log(0) = 0$ in information theory
		- $E(B_2) \approx 1.42$ (so round up to 2 bits)
#### Entropy for a 2D Example (Information Gain)
- ![[Pasted image 20230109210149.png|300]]
	- Red = 0.5, Blue & Yellow = 0.25, so E(S) = 3/2.
		- Round up and that means we need 2 bits to represent all possible outcomes.
	- 2-bits, so 2 Boolean Attributes
		- ![[Pasted image 20230109210746.png|100]]
		- We can split it using $A_1$ or $A_2$
			- ![[Pasted image 20230118173242.png|400]]
			- Information Gain($S, A_1$) = $E(S) - (|S_{1a}/S_{1}| * E(S_{1a}) + |S_{1b}/S_{1}| * E(S_{1b}))$ = $3/2 - (1/2 * 1 + 1/2 * 1)$ = $1/2$
				- ![[Pasted image 20230109211351.png|150]] (left is $S_{1a}$ & right is $S_{1b}$)
			- Information Gain($S, A_2$)
				- ![[Pasted image 20230109211822.png|320]]
			- So we wanna split by $A_2$ b because 1 > 1/2
	- HOWEVER, if the feature/attribute has a high amount of values (e.g. the name of a person), use the Information Gain Ratio instead, as it like a weighted avg type beat.
		- ![[Pasted image 20230118003223.png|400]]
## Data Example: Will GF say yes to playing tennis? 
- ![[Pasted image 20230109203437.png|300]]	
- By Computing Entropy of "Play Tennis", we can use that to calculate the Information Gained for each 4 Attributes.
	- ![[Pasted image 20230109212712.png|300]]
		- Outlook gave the highest gain for the root.
			- ![[Pasted image 20230118180520.png|300]]
		- The next split should be Humidity because the highest Information Gain
			- 
# Avoid Overfitting
- We can have noisy examples that throw of the trend in our data
	- e.g. (Sunny, Hot, Normal, Strong) = NO for tennis example
- We want the error of the entire distribution.
	- ![[Pasted image 20230109213259.png|220]]
- The hypothesis is overfitting if
	- ![[Pasted image 20230109213328.png|250]]
	- Random Diabetes Example:
		- ![[Pasted image 20230109213823.png|300]]
			- As the size of the tree increases, the accuracy of the data is surpassing the test data
	- When we detect overfitting, we should end the tree there (Decision Stump), using probability to create the leaves. 
		- ![[Pasted image 20230109214306.png|200]] (tennis example)
## Techniques to Address Overfitting Decision Trees
### Early Stopping (Online Pruning)
- Stop growing tree early (when u detect fall in accuracy of testing data)
### Post Pruning (Reduced-Error Pruning)
- Grow tree fully, then prune
- More costly but more accurate, because you have to backtrack to all nodes and prune
- Random Diabetes Data
	- ![[Pasted image 20230109215324.png|300]]
		- Started taking out nodes, until there was no more improvement.
#### Algorithm for Hyperparameter Tuning with Validation Data
- ![[Pasted image 20230109214838.png|300]]