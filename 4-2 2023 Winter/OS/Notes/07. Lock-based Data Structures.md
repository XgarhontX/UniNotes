 - We want to add locks to data structures to make them thread safe
	- Correctness, Performance, Lock Granularity?
# Counter
## w/o Locks
- Synchronization weary - not thread safe
- ![[Pasted image 20230202170209.png|250]]
	- We have a struct of the counter at the top
		- It has ++, & --
## w/ Locks (Thread Safe/Synchronized)
- ![[Pasted image 20230202170652.png|300]]
	- init() now has a accompanying lock
	- increment() has lock and unlock surrounding the critical section
- ![[Pasted image 20230202170636.png|300]]
	- decrement() got the same lock and unlocking surrounding the operation
	- In get(), the lock protect the create by copy (int rc = c->value) operation
		- Since rc is in the stack, we don't have to protect it
## Performance
- Performance of a 4 CPU core incrementing a single counter (focus on precise synchronized rn)
	- ![[Pasted image 20230202171636.png|200]]
		- The performance isn't scaling perfectly, it's slower the more threads
			- Because of waiting for the lock and context switching
			- The single counter doesn't allow any concurrency
## Sloppy Counter
- One counter per core, and a global counter.
	- Only the global counter has a lock
		- If you have multithreading in the local lock, then you also need local locks.
	- This allows for concurrent counting
- Sloppiness threshold (S)
	- After S updates in a core, it'll add it's value to the global counter
		- and reset to 0 to count up again.
		- This means the global counter will be a multiple of S
			- Don't count prime numbers then
	- Ex: S = 5
		- ![[Pasted image 20230202172805.png|300]]
	- The Sloppy Tradeoff
		- Lower S, more overhead checking
		- Higher S, less accurate counting
		- ![[Pasted image 20230202173126.png|200]]
# Linked List
## Single Lock
- ![[Pasted image 20230209154517.png|300]]
- ![[Pasted image 20230209154543.png|320]]![[Pasted image 20230209160710.png|320]]
	- The whole linked list has a lock
		- Lock when you manipulate it (e.g. insert)
	- For C, you have to malloc/calloc the Node struct and then manipulate addresses.
	- List_Insert() is adding to the head.
		- This is not cleanly written. It's better to use only one return, so that you are sure you have a unlock reached (unless you are god coder and make no mistakes)
			- Unlocking in an exception handler is considered a poor coding practice
		- The critical section is after malloc/calloc, it's when you assign the new head
	- List_Lookup() is aka O(n) search
		- You want to lock it still because you don't want another thread to edit it and remove stuff while you are searching
## Hand-Over-Hand Locking
- Introduce a lock for each node of a list
- Traversal involves handing over previous node’s lock, acquiring the next node’s lock
- Improves lock granularity
	- Degrades traversal performance
	- More concurrency
## Hybrid
- Nowadays, we do a balance between the coarse 1 lock and the very fine per node block
# Michael & Scott Concurrent Queues
- 2 locks
	- 1 at head &  1 at tail
	- Since that's where the critical section of read & write is at
- ![[Pasted image 20230209162411.png|320]]
	- When we initial the queue, we make a dummy and let head and tail point to it.
		- This is where the 2 locks are initialized
- ![[Pasted image 20230209162431.png|300]] 
	- Enqueue to the tail
		- First, malloc new node with value and pointing to null
		- Then lock the tail lock and assign the new node into the tail.
		- Unlock and return
- ![[Pasted image 20230209162506.png|250]]
	- Dequeue the head
		- First, lock the head lock.
		- Then, make reference to the current head node
			- (This implementation has the current head node as a dummy node. It is the head->next that is the actual head node.)
		- Get data, move head node reference over
		- Unlock and return
# Hash Table
- 
# Java Concurrent Data Structures
- java.util.concurrent.atomic package has data structures with automatic locking