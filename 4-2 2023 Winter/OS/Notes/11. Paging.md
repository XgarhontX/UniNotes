# Paging
- **Divide** memory up **into fixed-size pages**
	- Eliminates external fragmentation w/ segmentation
- Map virtual pages to physical page frames
	- Each process has separate mapping, called a page table
- ![[Pasted image 20230228155126.png|400]]
	- (pic is misleading, the left column of address should end at 63 and 127)
	- And there is a page table that maps the virtual pages to physical page frame.
		- ![[Pasted image 20230228155413.png|50]]
		- Then we use the virtual page frame number as index
			- Table remembers the "base" page frame
			- Virtual page number notes the index from that base
## Paging Data Structures
- Pages are fixed size
	- Virtual Address has 2 parts: virtual page number and offset
	- Example: Page Size: 16-bytes, Address Space: 64-bytes
		- ![[Pasted image 20230228155809.png|150]] (2nd page, 5th byte)
		- 2 bits for 4 possible pages
		- 4 bits for 64 bytes in that page
- Page Tables
	- Map virtual page number (**VPN**) to page frame number (**PFN**, Cheng was PPN)
		- VPN is the index into the table that determines PFN
		- PFN also called physical page number (PPN)
	- One page table entry (PTE) per page in virtual address space
## Page Lookup
- ![[Pasted image 20230228160338.png|400]]
## Example: Paging Address Translation
- Consider a 64-byte program address space (4 pages), stored in 128-byte physical memory (8 frames)
	- Page size = (size / \# of pages) = (64B / 4 pages) = <u>16B</u>
	- <u>VPN: 2 bits</u> for 4 pages &  <u>PPN: 4 bits</u> for 16 bytes
	- \# of page frame = (total page size / page size) = (128B / 16B) = 8 frames
		- \# of bits for PFN = $\log_2(8)$= 3
- ![[Pasted image 20230228160843.png|143]]![[Pasted image 20230228160829.png|250]]
	- VP1 is mapped to PF7, meaning PPN = 111
	- And you just copy the offset
## Paging Advantages
- Easy to allocate memory
	- List of chunks are easy to managed, bite size.
	- Allocating a page is adding/removing from the list
	- External fragmentation is not a problem, page to page is continuous
	- No need to track direction (i.e. stack is reversed)
## Paging Limitations
- Internal fragmentation
- Internal waste, if the memory within a page doesn't use all of the page
	- Process may not use memory in multiples of a page
- Management overhead, must O(n) to find space
- Page table is in memory, fetching it add an extra RAM I/O interrupt
	- 2 or more references per address lookup (page table, then memory)
	- (Solution â€“ use a hardware cache of lookups (TLB, more later))
- Memory required to hold page table can be significant
	- The smaller the page size and the higher the memory size, you will more table entries.
### Paging System Example
- How huge can a page table be?
- Consider a 32-bit computer system (up to 4GB memory), assume each page entry is of 32 bits (4B)
	- With a 4096-byte page size (4KB) (typical today, people based it on HDD block size)
	- \# of pages = (total size / page size) = (4GB / 4KB) = ($2^{22}$KB / $2^2$ KB) = $2^{20}$ pages
		- \# bits for PFN = $\log_2(2^{20})$ = 20
		- in this example, let it be the same for VPN too
			- 	- ![[Pasted image 20230228163751.png|100]]
	- \# bits for offset = $\log2(\text{page size})$ = $\log2(4\text{KB})$ = 12
	- Total size of the page table = (size of page table \* page size) = $(2^{20}$ \* 4KB)
	- \# processes w/ size of 1 page table to fill the entire memory = (total mem size / page size) = (4GB / 4MB) = 1024 processes
		- This sucks, as the process's size is usually more than 1 page table worth
### Paging: Use Larger Pages?
- Larger page size? Let's see 16KB = $2^{14}$
- With a 32 bit address space: $2^{32}$ = 4GB
- So the \# of pages = (total size / page size) = 4GB / 16KB = $2^{18}$ pages = 262,144 pages
	- ![[Pasted image 20230228170725.png|200]]
- Memory requirement cut to 1/4
	- But then the pages are bigger, meaning more internal fragmentation.
	- So no, the common page size = 4KB
	- What we need to do is use smaller tables
## What happens in Page Table?
- We only need 20 bits for the PFN for 32-bit systems, so we can use those extras as flags.
- x86
	- ![[Pasted image 20230228171317.png|300]]
### PTE: Common Flags
- ![[Pasted image 20230228171425.png|300]]
	- CHENG
# Multi-Level Page Tables
## Managing Page Tables
- Size of the page table for a 32-bit address space w/ 4K pages is 4MB
	- Too much overhead, < 1024 processes
- How do we reduce it?
	- Well, we only need to map the portion of the address space actually being used (tiny fraction of entire address space)
## Two-Level Page Table
- Virtual addresses (VAs) have three parts:
	- Master page number, secondary page number, and offset
		- Master page table maps VAs to secondary page table
		- Secondary page table maps page number to physical page
		- Offset indicates where in physical page address is located
	- ![[Pasted image 20230228172642.png|400]]
- With this, you don't have to allocate all pages at once in the beginning.
	- ![[Pasted image 20230228173227.png|450]]
	- Even though filling up all of the master and all frames makes the memory usage higher than linear paging, it is worth it for every other case which is way more common.
	- Master will need another array of pointers because of the needed allocation
	- Linux has 4 to 5 levels
### Example
- 16KB address space, 64B pages
	- ![[Pasted image 20230228173525.png|300]]
		- ![[Pasted image 20230228173855.png|400]]
	- Since we have 256 pages, $\log_{2}(256)=8$ is the amount of bits to index all pages. This is the PFN & VPN bit amount.
	- And since we have 64B page size, $\log_{2}(64)=6$ bits needed to index all bytes within a page. This is the Offset bit amount.
	- Extra: If the PTEs become 2B each (i.e. adding 8 more status bits), the page table size will just double to 128B while still maintaining a PTE count of 256.
	- Extra: If we move to a 2-level paging system where each master "chapter" can hold 32 PTEs, we will need (256 PTEs/32 PTEs per chapter) = 8 master pages or "chapters".
		- The VA will need another component. Master (Page Directory), Secondary (Page Frame), and Offset (for RAM location)
			- We need 3 bits for 8 master pages
			- We need 5 bits for 32 PTEs
			- We need 6 bits for 64 offsets in physical mem
		- So we would need 8 secondary pages + 1 master page = 9 pages maximum stored
			- In real world situation, it is not easy to reach all pages used.
## Dis/Advantage
### Advantages
- Only allocates page table space in proportion to the address space actually used
- Can easily grab next free page to expand page table
### Disadvantages
- Multi-level page tables are an example of a time-space tradeoff
- Sacrifice address translation time (now 2-level) for space
- Complexity: multi-level schemes are more complex
# Does Paging Make the System Too Slow?
- Each memory address translation for paging requires an extra or more memory reference.
	- Master, Secondary, maybe even more subsets, then to Physical w/ offset.
## Counting memory accesses
- ![[Pasted image 20230302172107.png|300]]
	- Each line is a memory address access to fetch instruction
	- movl, cmpl, and jne all have memory access too
### Visualizing  Memory Accesses:
- ![[Pasted image 20230302172514.png|300]]
	- 3 graphs for 3 locations
		- Page Table
		- Array
		- Code
	- This is only for the first 5 iterations.
	- There are 50 accesses (10 each iterations)
## Efficient Translations
- 1-Level page table already doubled the memory access count
- 2-Level and the cost triples.
	- ![[Pasted image 20230302172756.png|300]]
- We have to cache stuff
	- Translation Lookaside Buffer (TLB), which caches the PTEs
	- Cache translations in hardware, TLB managed by Memory Management Unit (MMU)
# TLB
- Translate virtual page numbers into PTEs (not physical address) for hashing
- TLBs implemented in hardware
	- Cache tags are virtual page numbers (indices)
	- Cache values are PTEs (entries from page tables)
	- With PTE + offset, can directly calculate physical address
## Procedure
- ![[Pasted image 20230302173351.png|500]]
## Managing TLBs
- Address translations for most instructions are handled using the TLB
	- >99% of translations, but there are misses (TLB miss) 
- When the TLB misses and a new PTE has to be loaded, a cached PTE must be evicted
	- Which one though? Choosing PTE to evict is called the TLB replacement policy
	- Implemented in hardware and is often simple (e.g., Last-Not-Used)
- ![[Pasted image 20230302173733.png|400]]
## Key Steps of TLB
- When TLB misses, the correct page is read from RAM and replaces a TLB entry first, then is read from TLB requery.
- All address translation goes through TLB.
## TLB Example
- Program address space: 256-byte
	- Addressable using 8 total bits ($2^8$)
	- 4 bits for the VPN ($2^4$ =16 total pages)
		- So Page size: 16 bytes
		- Offset is addressable using 4-bits
	- Store an array: of 10 4-byte integers
	- ![[Pasted image 20230307155011.png|200]]
- ![[Pasted image 20230307154936.png|200]]
	- When the program starts, the TLB doesn't know where a\[\] is.
	- Consider the access TLB
		- **a\[0\]**, a\[1\], a\[2\], **a\[3\]**, a\[4\], a\[5\], a\[6\], **a\[7\]**, a\[8\], a\[9\]
			- bold are misses
		- Hit amount: 7
		- Miss amount: 3
		- Hit rate: 3/7 = 70%
			- In reality, a program executes way longer, allowing for more hits, usually > 99%.
		- How many times that the memory is access: 13 (3 for page misses, 10 for values access)
	- Consider accessing a\[0\], a\[3\], a\[0\]
		- If the size of TLB = 1 slot, you will miss 3 times.
## TLB Address Translation
- ![[Pasted image 20230307161642.png|400]]
	- Page Fault is when you load up a page that was offloaded from memory to storage (SWAP)